{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script gets financial news data + stock price data from \n",
    "- Yahoo Finance News (scraping for more and older news)\n",
    "- Yahoo Finance RSS (get more recent news)\n",
    "- Google News (scraping for older news)\n",
    "- SEC (filings for finance reports and major event)\n",
    "- yfinance (for stock price data)\n",
    "\n",
    "It stores data into CSV (for FinBERT use) & JSONL (for Deepseek-r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import requests\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\"\"\"\n",
    "Stock Tickers selected for training and prediction:\n",
    "    General Market Tickers (for market-wide behavior prediction)\n",
    "        - SPY (S&P 500 ETF)\n",
    "        - DIA (Dow Jones)\n",
    "        - QQQ (Nasdaq 100)\n",
    "    Individual Stocks (for top company stock predictions)\n",
    "        - Tech: AAPL (Apple) / MSFT (Microsoft) / NVDA (Nvidia) / GOOGL (Google) /  SAP (German internation stock) / META\n",
    "        - Retail: TSLA (TESLA) / AMZN (AMAZON)\n",
    "        - FINANCE: JPM (JP MORGAN) / GS (GOLDMAN SACHS) / MS (Morgan Stanley)\n",
    "        - Healthcare : UNH (UnitedHealth)\n",
    "        - MARKET MOVER (WARREN BUFFET HOLDING COMPANY) : BRK.B (BERKSHIRE HATHAWAY)\n",
    "\"\"\"\n",
    "\n",
    "STOCK_TICKERS = [\"SPY\", \"QQQ\", \"DIA\", # General Market Indices\n",
    "                 \"AAPL\", \"MSFT\", \"NVDA\",\"GOOGL\", \"SAP\", \"META\", \"PLTR\", \"AMD\", \"INTC\", #Tech\n",
    "                 \"TSLA\", \"AMZN\", # tech/retail\n",
    "                 \"WMT\", \"COST\", #Consumer retail\n",
    "                 \"JPM\", \"GS\", \"MS\", \"HOOD\", # finance\n",
    "                 \"UNH\", \"PFE\", \"TEM\", # Health\n",
    "                 \"ACN\", \"IBM\", # Consulting\n",
    "                 \"XOM\", \"SHEL\", \"CVX\",\n",
    "                 \"BRK.B\" # others\n",
    "NEWS_YEARS_BACK = 5  # How many years of news history to fetch\n",
    "STOCK_YEARS_BACK = 10  # How many years of stock data to fetch\n",
    "\n",
    "\n",
    "YAHOO_FINANCE_NEWS_URL = \"https://finance.yahoo.com/quote/{ticker}/news\"\n",
    "YAHOO_FINANCE_RSS_URL = \"https://feeds.finance.yahoo.com/rss/2.0/headline?s={ticker}&region=US&lang=en-US\"\n",
    "GOOGLE_NEWS_SEARCH_URL = \"https://www.google.com/search?q={query}+stock+news&tbm=nws&tbs=cdr:1,cd_min:{start},cd_max:{end}\"\n",
    "SEC_FILINGS_URL = \"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={ticker}&type=8-K&count=50\"\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "def scrape_yahoo_news(ticker):\n",
    "    \"\"\"Scrapes Yahoo Finance's historical news for a given stock ticker.\"\"\"\n",
    "    url = YAHOO_FINANCE_NEWS_URL.format(ticker=ticker)\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    news_data = []\n",
    "    articles = soup.find_all(\"li\", class_=\"js-stream-content\")\n",
    "    \n",
    "    num_articles = min(50, len(articles))\n",
    "    for article in articles[:num_articles]:\n",
    "        try:\n",
    "            headline = article.find(\"h3\").text.strip()\n",
    "            link = \"https://finance.yahoo.com\" + article.find(\"a\")[\"href\"]\n",
    "            date_tag = article.find(\"time\")\n",
    "            date_published = date_tag[\"datetime\"][:10] if date_tag else \"unknown\"\n",
    "            news_data.append({\"date\": date_published, \"headline\": headline, \"url\": link})\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return news_data\n",
    "\n",
    "def fetch_yahoo_rss_news(ticker):\n",
    "    \"\"\"Fetches the latest financial news from Yahoo Finance's RSS feed.\"\"\"\n",
    "    url = YAHOO_FINANCE_RSS_URL.format(ticker=ticker)\n",
    "    feed = feedparser.parse(url)\n",
    "    \n",
    "    news_data = []\n",
    "    num_entries = min(50, len(feed.entries))\n",
    "    for entry in feed.entries[:num_entries]:\n",
    "        news_data.append({\"date\": entry.published[:10], \"headline\": entry.title, \"url\": entry.link})\n",
    "    \n",
    "    return news_data\n",
    "\n",
    "def scrape_google_news(ticker):\n",
    "    \"\"\"Scrapes Google News for historical articles (up to NEWS_YEARS_BACK).\"\"\"\n",
    "    news_data = []\n",
    "    end_date = datetime.today()\n",
    "    start_date = end_date - timedelta(days=365 * NEWS_YEARS_BACK)\n",
    "\n",
    "    search_url = GOOGLE_NEWS_SEARCH_URL.format(\n",
    "        query=ticker,\n",
    "        start=start_date.strftime(\"%m/%d/%Y\"),\n",
    "        end=end_date.strftime(\"%m/%d/%Y\")\n",
    "    )\n",
    "    \n",
    "    response = requests.get(search_url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"BNeawe vvjwJb AP7Wnd\")\n",
    "    num_articles = min(50, len(articles))\n",
    "    for article in articles[:num_articles]:  # Limit to 30 news articles\n",
    "        try:\n",
    "            headline = article.text.strip()\n",
    "            news_data.append({\"date\": \"unknown\", \"headline\": headline, \"url\": \"Google News\"})\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return news_data\n",
    "\n",
    "def scrape_sec_filings(ticker):\n",
    "    \"\"\"Scrapes SEC 8-K filings for earnings reports and major events.\"\"\"\n",
    "    url = SEC_FILINGS_URL.format(ticker=ticker)\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    news_data = []\n",
    "    for row in soup.find_all(\"tr\")[1:10]:  # Get the latest 10 filings\n",
    "        try:\n",
    "            cells = row.find_all(\"td\")\n",
    "            date_published = cells[3].text.strip()\n",
    "            link = \"https://www.sec.gov\" + cells[1].find(\"a\")[\"href\"]\n",
    "            news_data.append({\"date\": date_published, \"headline\": f\"SEC Filing for {ticker}\", \"url\": link})\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return news_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File names\n",
    "finbert_csv_file = \"finbert_training_data.csv\"\n",
    "deepseek_jsonl_file = \"deepseek_training_data.jsonl\"\n",
    "\n",
    "# Open CSV for FinBERT training\n",
    "with open(finbert_csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    fieldnames = [\"ticker\", \"date\", \"text\", \"label\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"price_change\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Open JSONL for Deepseek-r1 training\n",
    "    with open(deepseek_jsonl_file, \"w\", encoding=\"utf-8\") as jsonlfile:\n",
    "\n",
    "        for ticker in STOCK_TICKERS:\n",
    "            print(f\"Fetching news & stock data for {ticker}...\")\n",
    "\n",
    "            # Scrape Yahoo News\n",
    "            yahoo_news = scrape_yahoo_news(ticker)\n",
    "\n",
    "            # Fetch Yahoo RSS News\n",
    "            rss_news = fetch_yahoo_rss_news(ticker)\n",
    "\n",
    "            # Scrape Google News\n",
    "            google_news = scrape_google_news(ticker)\n",
    "\n",
    "            # Scrape SEC Filings\n",
    "            sec_news = scrape_sec_filings(ticker)\n",
    "\n",
    "            # Merge all sources\n",
    "            all_news = yahoo_news + rss_news + google_news + sec_news  \n",
    "\n",
    "            # Fetch Stock Price Data\n",
    "            stock_data = yf.Ticker(ticker).history(period=f\"{STOCK_YEARS_BACK}y\")\n",
    "\n",
    "            # Process news articles\n",
    "            for item in all_news:\n",
    "                date_published = item[\"date\"]\n",
    "                news_text = item[\"headline\"]\n",
    "                sentiment = \"neutral\"  # Placeholder for sentiment analysis\n",
    "\n",
    "                # Find stock data for the same date\n",
    "                if date_published in stock_data.index:\n",
    "                    stock_info = stock_data.loc[date_published]\n",
    "                    open_price = stock_info[\"Open\"]\n",
    "                    close_price = stock_info[\"Close\"]\n",
    "                    price_change = round(((close_price - open_price) / open_price) * 100, 2)\n",
    "\n",
    "                    # Save to FinBERT CSV\n",
    "                    writer.writerow({\n",
    "                        \"ticker\": ticker,\n",
    "                        \"date\": date_published,\n",
    "                        \"text\": news_text,\n",
    "                        \"label\": sentiment,\n",
    "                        \"open\": open_price,\n",
    "                        \"high\": stock_info[\"High\"],\n",
    "                        \"low\": stock_info[\"Low\"],\n",
    "                        \"close\": close_price,\n",
    "                        \"volume\": stock_info[\"Volume\"],\n",
    "                        \"price_change\": price_change\n",
    "                    })\n",
    "\n",
    "                    # Save to Deepseek-r1 JSONL\n",
    "                    json_data = {\n",
    "                        \"ticker\": ticker,\n",
    "                        \"date\": date_published,\n",
    "                        \"title\": item[\"headline\"],\n",
    "                        \"summary\": news_text,\n",
    "                        \"sentiment\": sentiment,\n",
    "                        \"open\": open_price,\n",
    "                        \"high\": stock_info[\"High\"],\n",
    "                        \"low\": stock_info[\"Low\"],\n",
    "                        \"close\": close_price,\n",
    "                        \"volume\": stock_info[\"Volume\"],\n",
    "                        \"price_change\": price_change\n",
    "                    }\n",
    "                    jsonlfile.write(json.dumps(json_data) + \"\\n\")\n",
    "\n",
    "            time.sleep(3)\n",
    "\n",
    "print(f\"FinBERT training data saved to {finbert_csv_file}\")\n",
    "print(f\"Deepseek-r1 training data saved to {deepseek_jsonl_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
