{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning using dora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model: deepseek\n",
    "Dataset: financialPhraseBank - from finbert \n",
    "Techniques: dora\n",
    "- data augmentation (to consider if data is too little)\n",
    "Using ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback available to show.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 35519 train examples to data/sentiment_data/train.csv\n",
      "Wrote 4217 validation examples to data/sentiment_data/validation.csv\n",
      "Wrote 4846 test examples to data/sentiment_data/test.csv\n"
     ]
    }
   ],
   "source": [
    "%tb\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizerFast,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "AutoConfig\n",
    ")\n",
    "import torch\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = (\n",
    "    \"/home/m/mehrad/brikiyou/scratch/ift6289/IFT6289-project/data/\"\n",
    "    \"FinancialPhraseBank-v1.0\"\n",
    ")\n",
    "out_dir = \"data/sentiment_data\"\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "train_files = [\n",
    "    \"Sentences_75Agree_utf8.txt\",\n",
    "    \"Sentences_AllAgree_utf8.txt\",\n",
    "    \"augmented_Sentences_66Agree_utf8.txt\",\n",
    "    \"augmented_Sentences_AllAgree_utf8.txt\",\n",
    "    \"augmented_Sentences_75Agree_utf8.txt\"\n",
    "]\n",
    "val_file = \"Sentences_66Agree_utf8.txt\"\n",
    "test_file = \"Sentences_50Agree_utf8.txt\"\n",
    "\n",
    "def load_fpbank(path):\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        sep=\"\\t\",\n",
    "        names=[\"text\", \"label\"],\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "train_dfs = []\n",
    "for fname in train_files:\n",
    "    full = os.path.join(data_dir, fname)\n",
    "    train_dfs.append(load_fpbank(full))\n",
    "train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "\n",
    "valid_df = load_fpbank(os.path.join(data_dir, val_file))\n",
    "test_df  = load_fpbank(os.path.join(data_dir, test_file))\n",
    "\n",
    "train_path = os.path.join(out_dir, \"train.csv\")\n",
    "valid_path = os.path.join(out_dir, \"validation.csv\")\n",
    "test_path  = os.path.join(out_dir, \"test.csv\")\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "valid_df.to_csv(valid_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"Wrote {len(train_df)} train examples to {train_path}\")\n",
    "print(f\"Wrote {len(valid_df)} validation examples to {valid_path}\")\n",
    "print(f\"Wrote {len(test_df)} test examples to {test_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING STARTS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport wandb\\nwandb.login()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import wandb\n",
    "wandb.login()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_HUB_ENABLE_HF_TRANSFER=True\n"
     ]
    }
   ],
   "source": [
    "%env HF_HUB_ENABLE_HF_TRANSFER=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8e5130b84548ba9a0770e0a69dd18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5072245e459d4657bb30aaa05e4a2588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971703edfaac41a0a5b2c7a2037cdfaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "cache_dir = \"/home/m/mehrad/brikiyou/scratch/huggingface_cache\"\n",
    "\n",
    "import os\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = \"/home/m/mehrad/brikiyou/scratch/triton_cache\"\n",
    "os.environ[\"TRITON_HOME\"]      = \"/home/m/mehrad/brikiyou/scratch/triton_home\"\n",
    "os.environ[\"CC\"]  = \"/scinet/balam/rocky9/software/2023a/opt/cuda-12.3.1/gcc/12.3.0/bin/gcc\"\n",
    "os.environ[\"CXX\"] = \"/scinet/balam/rocky9/software/2023a/opt/cuda-12.3.1/gcc/12.3.0/bin/g++\"\n",
    "os.environ[\"HF_HOME\"]            = \"/home/m/mehrad/brikiyou/scratch/huggingface_cache\"\n",
    "os.environ[\"HF_HUB_CACHE\"]       = os.path.join(os.environ[\"HF_HOME\"], \"hub\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(os.environ[\"HF_HOME\"], \"models\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"]  = os.path.join(os.environ[\"HF_HOME\"], \"datasets\")\n",
    "\n",
    "cache_dir = os.environ[\"HF_HOME\"]\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"data/sentiment_data/train.csv\",\n",
    "    \"validation\": \"data/sentiment_data/validation.csv\",\n",
    "    \"test\": \"data/sentiment_data/test.csv\",\n",
    "}\n",
    "ds = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=data_files,\n",
    "    cache_dir=\"/home/m/mehrad/brikiyou/scratch/huggingface_cache/datasets\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 30] Read-only file system: '/home/m/mehrad/brikiyou/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B'\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcb45a5d0c640908fa5e953c5461fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, \n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    #quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    #trust_remote_code=False\n",
    "    id2label={i: l for i, l in enumerate(ds[\"train\"].unique(\"label\"))},\n",
    "    label2id={l: i for i, l in enumerate(ds[\"train\"].unique(\"label\"))},\n",
    "\n",
    "    torch_dtype=torch.bfloat16, # if A100 GPU Available     \n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "# Look at tokenizer - to adapt to dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=cache_dir,\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for Dora fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable() # comment out if saving vram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(152064, 3584)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\", # To look for for self attention & mlp without act_fn\n",
    "                    \"lora_magintude_vector\"], # DoRa \n",
    "    lora_dropout=0.01, # To tune\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    use_dora=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cf2b4956ff45bb8d876e11c98310ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/35519 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc79274d3aa541519b03b0b86ade9235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea2f38b974740128a04fbf1abefaa3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Both `tokenizer` and `processing_class` are set for `Trainer.__init__`. Using `processing_class=<class 'transformers.data.data_collator.DataCollatorWithPadding'>` and ignoring deprecated `tokenizer=LlamaTokenizerFast(name_or_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', vocab_size=151643, model_max_length=16384, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<｜end▁of▁sentence｜>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t151643: AddedToken(\"<｜end▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151644: AddedToken(\"<｜User｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151645: AddedToken(\"<｜Assistant｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151646: AddedToken(\"<｜begin▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151647: AddedToken(\"<|EOT|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151648: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151649: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151665: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: accuracy}\n\u001b[32m     49\u001b[39m training_args = TrainingArguments(\n\u001b[32m     50\u001b[39m     output_dir=out_dir,\n\u001b[32m     51\u001b[39m     do_train=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     63\u001b[39m     hub_token=\u001b[33m\"\u001b[39m\u001b[33mhf_NZeQuaGsrdzNopapERdUXnASHxmkiuubsr\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     64\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m trainer = \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalidation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDataCollatorWithPadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m trainer.train()\n\u001b[32m     78\u001b[39m trainer.save_model(out_dir)                \n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/fs0/scratch/m/mehrad/brikiyou/ift6289/IFT6289-project/project_venv_2/lib/python3.11/site-packages/transformers/utils/deprecation.py:166\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# raise error or notify user\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m minimum_action == Action.RAISE:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# If we are compiling, we do not raise the warning as it would break compilation\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Both `tokenizer` and `processing_class` are set for `Trainer.__init__`. Using `processing_class=<class 'transformers.data.data_collator.DataCollatorWithPadding'>` and ignoring deprecated `tokenizer=LlamaTokenizerFast(name_or_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', vocab_size=151643, model_max_length=16384, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<｜end▁of▁sentence｜>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t151643: AddedToken(\"<｜end▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151644: AddedToken(\"<｜User｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151645: AddedToken(\"<｜Assistant｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151646: AddedToken(\"<｜begin▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151647: AddedToken(\"<|EOT|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151648: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151649: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n\t151665: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n)`."
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_METRICS_CACHE\"]  = os.path.join(os.environ[\"HF_HOME\"], \"evaluate\")\n",
    "hub_model_id = \"brikiyou/fpbank-deepseek-8b-sentiment-analysis\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    config=AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    ",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=cache_dir,\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    out = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    out[\"labels\"] = batch[\"label\"]\n",
    "    return out\n",
    "\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    ds[split] = ds[split].map(\n",
    "        tokenize_batch,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\",\"label\"],\n",
    "        num_proc=4,\n",
    "    )\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_steps=100,\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    push_to_hub=False, # put true\n",
    "    hub_model_id=hub_model_id,\n",
    "    hub_token=\"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\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    #processing_class=DataCollatorWithPadding,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(out_dir)                \n",
    "tokenizer.save_pretrained(out_dir)          \n",
    "#trainer.push_to_hub()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ift6289-project",
   "language": "python",
   "name": "projectift6289"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
